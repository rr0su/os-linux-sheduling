# ðŸ§  Scheduling Mini Project  
### **Latency Benchmarking & Scheduling Analysis (Linux / WSL2 / Ubuntu)**

---

## ðŸŽ¯ Objective

Measure and compare **CPU scheduling latency** under different scenarios (baseline, pinned, background load, RT priority, etc.) using `cyclictest`, visualize results, and analyze how Linux scheduling policies affect performance and responsiveness.

---

## ðŸ§© Overview

**Goal:** Evaluate how CPU affinity, background load, and real-time priority affect timer latency.

**Tools used:**
- `cyclictest` (from `rt-tests`) â€” measures latency of periodic tasks  
- `stress` â€” generates CPU load  
- `taskset` / `chrt` â€” control CPU affinity and scheduling policy  
- `perf` â€” optional deeper scheduling trace  
- `Python (pandas, matplotlib)` â€” for CSV parsing and graph plotting  

---

## âš™ï¸ Environment Setup

Run these once:

```bash
sudo apt update
sudo apt install -y rt-tests stress taskset python3 python3-pip linux-tools-common linux-tools-$(uname -r) perf
pip3 install pandas matplotlib
```




Check environment info (for report):
```bash
uname -a
lscpu | grep 'Model name'
nproc
```
------------------

ðŸ“‚ Project Structure
Day17_Scheduling_Project/
â”‚
â”œâ”€â”€ run_latency_tests.sh        # Runs all benchmark scenarios
â”œâ”€â”€ parse_cyclic_outputs.py     # Extracts min/avg/max latency from outputs
â”œâ”€â”€ plot_latency.py             # Generates bar chart (latency_summary.png)
â”‚
â””â”€â”€ latency_results/
    â”œâ”€â”€ baseline.txt
    â”œâ”€â”€ pinned_cpu0.txt
    â”œâ”€â”€ with_background_load.txt
    â”œâ”€â”€ pinned_cpu0_with_load.txt
    â”œâ”€â”€ chrt_rt.txt
    â”œâ”€â”€ summary_latency.csv
    â””â”€â”€ latency_summary.png
    
--------------------------------------------

ðŸ§ª Step-by-Step Procedure
1ï¸âƒ£ Run All Benchmarks
```bash
chmod +x run_latency_tests.sh
sudo ./run_latency_tests.sh
```
This creates the folder latency_results/ containing text outputs for each scenario.


2ï¸âƒ£ Parse Results into CSV
```bash
chmod +x parse_cyclic_outputs.py
./parse_cyclic_outputs.py
```

âž¡ Produces: latency_results/summary_latency.csv


3ï¸âƒ£ Generate Graph
```bash
chmod +x plot_latency.py
./plot_latency.py
```

âž¡ Produces: latency_results/latency_summary.png

4ï¸âƒ£ (Optional) Capture Scheduler Trace
```bash
sudo perf sched record -a sleep 10
sudo perf sched latency > latency_results/perf_sched_latency.txt
```

ðŸ“Š Expected Results (Example)
| Scenario              | Min (Âµs) | Avg (Âµs) | Max (Âµs) |
| --------------------- | -------- | -------- | -------- |
| baseline              | 5        | 8        | 25       |
| pinned_cpu0           | 3        | 6        | 18       |
| with_background_load  | 7        | 15       | 60       |
| pinned_cpu0_with_load | 5        | 10       | 40       |
| chrt_rt               | 2        | 3        | 10       |

ðŸ“ˆ See the generated graph in latency_results/latency_summary.png.

--------------------

ðŸ§  Analysis Guidelines

Write this section in your report (200â€“400 words):
    .Effect of CPU pinning: Did latency variance (max) reduce? Why?
    .Effect of background load: How much did max latency rise?
    .Effect of RT priority: How does SCHED_FIFO/SCHED_RR scheduling improve determinism?
    .Trade-offs: Low latency vs fairness vs throughput.
    .If CONFIG_PREEMPT or low-latency kernel used: Note differences.
    
  Example key findings:
           .Pinned threads â†’ better cache locality, lower jitter.
          .Background load â†’ higher scheduling delays.
          .RT policy â†’ near-constant latency but starves normal tasks.

ðŸ“˜ Deliverables
| File                        | Description                  |
| --------------------------- | ---------------------------- |
| `run_latency_tests.sh`      | Benchmark automation         |
| `latency_results/*.txt`     | Raw cyclictest logs          |
| `summary_latency.csv`       | Parsed latency data          |
| `latency_summary.png`       | Graph visualization          |
| `report.md` or `report.pdf` | Final analysis & conclusions |

